f1:
movq $2, %rax
ret
f2:
movq $2, %r11
cmp $10, %rdi
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f3:
movq $2, %r11
cmp $10, %rdi
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f4:
movq $2, %rax
ret
f5:
movq $2, %rax
ret
f6:
testq $1, %rdi
jnz raisesig
leaq 2(%rdi), %rax
leaq 2(%rdi), %rsi
movq $2, %r11
cmp %rax, %rsi
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f7:
testq $1, %rdi
jnz raisesig
movq %rdi, %rsi
xorq $2, %rsi
movq $2, %r11
cmp %rdi, %rsi
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f8:
testq $1, %rdi
jnz raisesig
movq %rdi, %rax
xorq $2, %rax
movq $2, %r11
cmp $10, %rax
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f9:
testq $1, %rdi
jnz raisesig
leaq (, %rdi, 2), %rax
movq $2, %r11
cmp $20, %rax
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f10:
movq $0, (%r15)
movq $0, 8(%r15)
movq %r15, %rax
addq $16, %r15
addq $1, %rax
movq $0, (%r15)
movq $0, 8(%r15)
movq %r15, %rsi
addq $16, %r15
addq $1, %rsi
movq $2, %r11
cmp %rax, %rsi
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f11:
movq $0, (%r15)
movq $0, 8(%r15)
movq %r15, %rax
addq $16, %r15
addq $1, %rax
movq $2, %r11
cmp $2, %rax
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f12:
movq $0, (%r15)
movq $0, 8(%r15)
movq %r15, %rax
addq $16, %r15
addq $1, %rax
movq $2, %r11
cmp $2, %rax
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
f13:
movq %rdi, (%r15)
movq $2, 8(%r15)
movq %r15, %rax
addq $16, %r15
movq (%rax), %rax
movq $2, (%r15)
movq %rdi, 8(%r15)
movq %r15, %rsi
addq $16, %r15
movq 8(%rsi), %rsi
movq $2, %r11
cmp %rax, %rsi
leaq 0(, 1), %rax
cmovz %r11, %rax
ret
